{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurize passage text\n",
    "\n",
    "**Goal:** Given a passage of text, ultimately generate features that can be processed alongside transcripts and alignments to predict reading experts' **observations**.\n",
    "\n",
    "**Constraint:** features should be interpretable/explainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by processing one single passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ycm/opt/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import spacy\n",
    "import time\n",
    "import eng_to_ipa\n",
    "import pandas as pd\n",
    "from spacy import displacy\n",
    "from benepar.spacy_plugin import BeneparComponent\n",
    "from NgramFetcher import NgramFetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tRaw passage [incl. line breaks]\n",
      "Sam and Jo went for a hike. They took a path through the $woods. Suddenly, Sam heard a noise coming from the tree $above their heads. Jo climbed up to see what the noise was $and found two baby squirrels. The babies were alone, but $their mother must be somewhere near. The children watched $and waited.$$Sure enough, the mother soon returned with a mouthful of nuts. $The noises stopped as the baby squirrels began to eat.$$Sam and Jo smiled, knowing the squirrels were safe with $their mother.#1#1.35#1#0.535\n",
      "\tProcessed passage\n",
      "Sam and Jo went for a hike. They took a path through the woods. Suddenly, Sam heard a noise coming from the tree above their heads. Jo climbed up to see what the noise was and found two baby squirrels. The babies were alone, but their mother must be somewhere near. The children watched and waited. Sure enough, the mother soon returned with a mouthful of nuts. The noises stopped as the baby squirrels began to eat. Sam and Jo smiled, knowing the squirrels were safe with their mother.\n"
     ]
    }
   ],
   "source": [
    "with open('data/moby-passages-36/passages-with-line-breaks.tsv') as f:\n",
    "    lines = (l.split('\\t') for l in f)\n",
    "    lines = {xy[0]: xy[1].strip() for xy in lines}\n",
    "\n",
    "line = lines['330']\n",
    "print('\\tRaw passage [incl. line breaks]', line, sep='\\n')\n",
    "line_text_only = line[:line.index('#')].replace('$', ' ').replace('  ', ' ')\n",
    "print('\\tProcessed passage', line_text_only, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constituency parsing\n",
    "\n",
    "Using an implementation of \"Constituency Parsing with a Self-Attentive Encoder\" (ACL 2018). Gives much better results than more rudimentary statistical parsers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(BeneparComponent('benepar_en2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (NNP Sam) (CC and) (NNP Jo)) (VP (VBD went) (PP (IN for) (NP (DT a) (NN hike)))) (. .))\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(line_text_only)\n",
    "sent = list(doc.sents)[0]\n",
    "const_parse = sent._.parse_string\n",
    "print(const_parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='misc/fig-parse-tree.png' width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"e5b43c91f2484b439a05f977e9f68a25-0\" class=\"displacy\" width=\"750\" height=\"287.0\" direction=\"ltr\" style=\"max-width: none; height: 287.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Sam</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"150\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"150\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"250\">Jo</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"250\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">went</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"550\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"550\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">hike.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e5b43c91f2484b439a05f977e9f68a25-0-0\" stroke-width=\"2px\" d=\"M70,152.0 C70,2.0 350.0,2.0 350.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e5b43c91f2484b439a05f977e9f68a25-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,154.0 L62,142.0 78,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e5b43c91f2484b439a05f977e9f68a25-0-1\" stroke-width=\"2px\" d=\"M70,152.0 C70,102.0 140.0,102.0 140.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e5b43c91f2484b439a05f977e9f68a25-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M140.0,154.0 L148.0,142.0 132.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e5b43c91f2484b439a05f977e9f68a25-0-2\" stroke-width=\"2px\" d=\"M70,152.0 C70,52.0 245.0,52.0 245.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e5b43c91f2484b439a05f977e9f68a25-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245.0,154.0 L253.0,142.0 237.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e5b43c91f2484b439a05f977e9f68a25-0-3\" stroke-width=\"2px\" d=\"M370,152.0 C370,102.0 440.0,102.0 440.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e5b43c91f2484b439a05f977e9f68a25-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M440.0,154.0 L448.0,142.0 432.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e5b43c91f2484b439a05f977e9f68a25-0-4\" stroke-width=\"2px\" d=\"M570,152.0 C570,102.0 640.0,102.0 640.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e5b43c91f2484b439a05f977e9f68a25-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570,154.0 L562,142.0 578,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e5b43c91f2484b439a05f977e9f68a25-0-5\" stroke-width=\"2px\" d=\"M470,152.0 C470,52.0 645.0,52.0 645.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e5b43c91f2484b439a05f977e9f68a25-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M645.0,154.0 L653.0,142.0 637.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(sent, style='dep', options={'distance': 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features pertaining to entire passage\n",
    "\n",
    " - length of passage\n",
    " - number of NPs\n",
    " - average number of syllables\n",
    " - average number of morphemes per word\n",
    " - number of distinct phonetic sounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features that pertain to a single token\n",
    "\n",
    " - POS\n",
    " - number of syllables\n",
    " - number of distinct phonetic sounds\n",
    " - left/right-distance to end of phrase (what constitutes *phrase*?)\n",
    " - function word? (and, to, etc.)\n",
    " - frequency over some corpus (a measure of how common a word is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading data/staging/ngram_freqs.json\n"
     ]
    }
   ],
   "source": [
    "fetcher = NgramFetcher('data/staging/ngram_freqs.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ngrams_for_sentence(doc, n_before, n_after):\n",
    "    sentences = [['_START_'] + [token.text for token in sent] for sent in doc.sents]\n",
    "    rv = []\n",
    "    for sent in sentences:\n",
    "        for idx, word in enumerate(sent):\n",
    "            query = ' '.join(sent[max(0, idx - n_before): idx + n_after + 1]).replace(',', '_._')\n",
    "            score, was_scraped = fetcher.fetch(query)\n",
    "            rv.append(tuple([word, score]))\n",
    "            if was_scraped:\n",
    "                time.sleep(5)\n",
    "    return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying: Jo went for\n",
      "Ngram not found for: Jo went for\n",
      "trying: Jo climbed up\n",
      "Ngram not found for: Jo climbed up\n",
      "trying: baby squirrels began\n",
      "Ngram not found for: baby squirrels began\n",
      "trying: knowing the squirrels\n",
      "Ngram not found for: knowing the squirrels\n",
      "trying: squirrels were safe\n",
      "Ngram not found for: squirrels were safe\n"
     ]
    }
   ],
   "source": [
    "rv = compute_ngrams_for_sentence(doc, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature: trigram likelihood\n",
    "\n",
    "- Likelihood of `center` given: `context` `center` `context`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_START_', 2.89174061585493),\n",
       " ('Sam', 0.08594705249092478),\n",
       " ('and', 0.0002478075859267613),\n",
       " ('Jo', 0.001364227892765734),\n",
       " ('went', 0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_trigram_likelihood = rv\n",
    "f_trigram_likelihood[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature: word length\n",
    "\n",
    "- number of letters\n",
    "- `_START_` token is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_START_', 0), ('Sam', 3), ('and', 3), ('Jo', 2), ('went', 4)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [['_START_'] + [token.text for token in sent] for sent in doc.sents]\n",
    "rv = []\n",
    "for sent in sentences:\n",
    "    for word in sent:\n",
    "        if word == '_START_':\n",
    "            rv.append(tuple([word, 0]))\n",
    "            continue\n",
    "        rv.append(tuple([word, len(word)]))\n",
    "f_word_length = rv\n",
    "f_word_length[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature: sight word\n",
    "- 1 if sight word AND at/below current level of passage\n",
    "- `_START_` token counts as sight word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/general-resources/dolch-sight-words.json') as f:\n",
    "    dolch = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "appropriate_sight_words = dolch['pre-primer'] + dolch['primer'] + dolch['first-grade'] + dolch['second-grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_START_', 1), ('Sam', 0), ('and', 1), ('Jo', 0), ('went', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rv = []\n",
    "for sent in sentences:\n",
    "    for word in sent:\n",
    "        if word == '_START_':\n",
    "            rv.append(tuple([word, 1]))\n",
    "            continue\n",
    "        if word in appropriate_sight_words:\n",
    "            rv.append(tuple([word, 1]))\n",
    "        else:\n",
    "            rv.append(tuple([word, 0]))\n",
    "f_sight_word = rv\n",
    "f_sight_word[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features: more ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to data/staging/ngram_freqs.json\n"
     ]
    }
   ],
   "source": [
    "fetcher.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv_ngrams_1_0 = compute_ngrams_for_sentence(doc, 1, 0) # bigram: v w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying: Jo went for\n",
      "Ngram not found for: Jo went for\n",
      "trying: Jo climbed up\n",
      "Ngram not found for: Jo climbed up\n",
      "trying: baby squirrels began\n",
      "Ngram not found for: baby squirrels began\n",
      "trying: knowing the squirrels\n",
      "Ngram not found for: knowing the squirrels\n",
      "trying: squirrels were safe\n",
      "Ngram not found for: squirrels were safe\n"
     ]
    }
   ],
   "source": [
    "rv_ngrams_2_0 = compute_ngrams_for_sentence(doc, 2, 0) # bigram: v v w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv_ngrams_0_1 = compute_ngrams_for_sentence(doc, 0, 1) # bigram: w v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying: Jo went for\n",
      "Ngram not found for: Jo went for\n",
      "trying: Jo climbed up\n",
      "Ngram not found for: Jo climbed up\n",
      "trying: baby squirrels began\n",
      "Ngram not found for: baby squirrels began\n",
      "trying: knowing the squirrels\n",
      "Ngram not found for: knowing the squirrels\n",
      "trying: squirrels were safe\n",
      "Ngram not found for: squirrels were safe\n"
     ]
    }
   ],
   "source": [
    "rv_ngrams_0_2 = compute_ngrams_for_sentence(doc, 0, 2) # bigram: w v v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying: _START_ Sam and Jo\n",
      "Ngram not found for: _START_ Sam and Jo\n",
      "trying: Sam and Jo went\n",
      "Ngram not found for: Sam and Jo went\n",
      "trying: and Jo went for\n",
      "Ngram not found for: and Jo went for\n",
      "trying: Jo went for a\n",
      "Ngram not found for: Jo went for a\n",
      "trying: _START_ They took a\n",
      "Ngram not found for: _START_ They took a\n",
      "trying: _START_ Suddenly _._ Sam\n",
      "Ngram not found for: _START_ Suddenly _._ Sam\n",
      "trying: Suddenly _._ Sam heard\n",
      "Ngram not found for: Suddenly _._ Sam heard\n",
      "trying: _._ Sam heard a\n",
      "Ngram not found for: _._ Sam heard a\n",
      "trying: Sam heard a noise\n",
      "Ngram not found for: Sam heard a noise\n",
      "trying: _START_ Jo climbed up\n",
      "Ngram not found for: _START_ Jo climbed up\n",
      "trying: Jo climbed up to\n",
      "Ngram not found for: Jo climbed up to\n",
      "trying: noise was and found\n",
      "Ngram not found for: noise was and found\n",
      "trying: was and found two\n",
      "Ngram not found for: was and found two\n",
      "trying: and found two baby\n",
      "Ngram not found for: and found two baby\n",
      "trying: found two baby squirrels\n",
      "Ngram not found for: found two baby squirrels\n",
      "trying: two baby squirrels .\n",
      "Ngram not found for: two baby squirrels .\n",
      "trying: _START_ The babies were\n",
      "Ngram not found for: _START_ The babies were\n",
      "trying: The babies were alone\n",
      "Ngram not found for: The babies were alone\n",
      "trying: babies were alone _._\n",
      "Ngram not found for: babies were alone _._\n",
      "trying: were alone _._ but\n",
      "Ngram not found for: were alone _._ but\n",
      "trying: alone _._ but their\n",
      "Ngram not found for: alone _._ but their\n",
      "trying: _._ but their mother\n",
      "Ngram not found for: _._ but their mother\n",
      "trying: but their mother must\n",
      "Ngram not found for: but their mother must\n",
      "trying: their mother must be\n",
      "trying: mother must be somewhere\n",
      "Ngram not found for: mother must be somewhere\n",
      "trying: must be somewhere near\n",
      "trying: be somewhere near .\n",
      "trying: _START_ The children watched\n",
      "Ngram not found for: _START_ The children watched\n",
      "trying: The children watched and\n",
      "trying: children watched and waited\n",
      "trying: watched and waited .\n",
      "trying: _START_ Sure enough _._\n",
      "Ngram not found for: _START_ Sure enough _._\n",
      "trying: Sure enough _._ the\n",
      "Ngram not found for: Sure enough _._ the\n",
      "trying: enough _._ the mother\n",
      "Ngram not found for: enough _._ the mother\n",
      "trying: _._ the mother soon\n",
      "Ngram not found for: _._ the mother soon\n",
      "trying: the mother soon returned\n",
      "Ngram not found for: the mother soon returned\n",
      "trying: mother soon returned with\n",
      "Ngram not found for: mother soon returned with\n",
      "trying: _START_ The noises stopped\n",
      "Ngram not found for: _START_ The noises stopped\n",
      "trying: The noises stopped as\n",
      "Ngram not found for: The noises stopped as\n",
      "trying: noises stopped as the\n",
      "Ngram not found for: noises stopped as the\n",
      "trying: stopped as the baby\n",
      "Ngram not found for: stopped as the baby\n",
      "trying: as the baby squirrels\n",
      "Ngram not found for: as the baby squirrels\n",
      "trying: the baby squirrels began\n",
      "Ngram not found for: the baby squirrels began\n",
      "trying: baby squirrels began to\n",
      "Ngram not found for: baby squirrels began to\n",
      "trying: squirrels began to eat\n",
      "Ngram not found for: squirrels began to eat\n",
      "trying: _START_ Sam and Jo\n",
      "Ngram not found for: _START_ Sam and Jo\n",
      "trying: Sam and Jo smiled\n",
      "Ngram not found for: Sam and Jo smiled\n",
      "trying: and Jo smiled _._\n",
      "Ngram not found for: and Jo smiled _._\n",
      "trying: Jo smiled _._ knowing\n",
      "Ngram not found for: Jo smiled _._ knowing\n",
      "trying: smiled _._ knowing the\n",
      "Ngram not found for: smiled _._ knowing the\n",
      "trying: _._ knowing the squirrels\n",
      "Ngram not found for: _._ knowing the squirrels\n",
      "trying: knowing the squirrels were\n",
      "Ngram not found for: knowing the squirrels were\n",
      "trying: the squirrels were safe\n",
      "Ngram not found for: the squirrels were safe\n",
      "trying: squirrels were safe with\n",
      "Ngram not found for: squirrels were safe with\n"
     ]
    }
   ],
   "source": [
    "rv_ngrams_1_2 = compute_ngrams_for_sentence(doc, 1, 2) # bigram: v w v v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying: _START_ Sam and Jo\n",
      "Ngram not found for: _START_ Sam and Jo\n",
      "trying: Sam and Jo went\n",
      "Ngram not found for: Sam and Jo went\n",
      "trying: and Jo went for\n",
      "Ngram not found for: and Jo went for\n",
      "trying: Jo went for a\n",
      "Ngram not found for: Jo went for a\n",
      "trying: _START_ They took a\n",
      "Ngram not found for: _START_ They took a\n",
      "trying: _START_ Suddenly _._ Sam\n",
      "Ngram not found for: _START_ Suddenly _._ Sam\n",
      "trying: Suddenly _._ Sam heard\n",
      "Ngram not found for: Suddenly _._ Sam heard\n",
      "trying: _._ Sam heard a\n",
      "Ngram not found for: _._ Sam heard a\n",
      "trying: Sam heard a noise\n",
      "Ngram not found for: Sam heard a noise\n",
      "trying: _START_ Jo climbed up\n",
      "Ngram not found for: _START_ Jo climbed up\n",
      "trying: Jo climbed up to\n",
      "Ngram not found for: Jo climbed up to\n",
      "trying: noise was and found\n",
      "Ngram not found for: noise was and found\n",
      "trying: was and found two\n",
      "Ngram not found for: was and found two\n",
      "trying: and found two baby\n",
      "Ngram not found for: and found two baby\n",
      "trying: found two baby squirrels\n",
      "Ngram not found for: found two baby squirrels\n",
      "trying: two baby squirrels .\n",
      "Ngram not found for: two baby squirrels .\n",
      "trying: _START_ The babies were\n",
      "Ngram not found for: _START_ The babies were\n",
      "trying: The babies were alone\n",
      "Ngram not found for: The babies were alone\n",
      "trying: babies were alone _._\n",
      "Ngram not found for: babies were alone _._\n",
      "trying: were alone _._ but\n",
      "Ngram not found for: were alone _._ but\n",
      "trying: alone _._ but their\n",
      "Ngram not found for: alone _._ but their\n",
      "trying: _._ but their mother\n",
      "Ngram not found for: _._ but their mother\n",
      "trying: but their mother must\n",
      "Ngram not found for: but their mother must\n",
      "trying: mother must be somewhere\n",
      "Ngram not found for: mother must be somewhere\n",
      "trying: _START_ The children watched\n",
      "Ngram not found for: _START_ The children watched\n",
      "trying: _START_ Sure enough _._\n",
      "Ngram not found for: _START_ Sure enough _._\n",
      "trying: Sure enough _._ the\n",
      "Ngram not found for: Sure enough _._ the\n",
      "trying: enough _._ the mother\n",
      "Ngram not found for: enough _._ the mother\n",
      "trying: _._ the mother soon\n",
      "Ngram not found for: _._ the mother soon\n",
      "trying: the mother soon returned\n",
      "Ngram not found for: the mother soon returned\n",
      "trying: mother soon returned with\n",
      "Ngram not found for: mother soon returned with\n",
      "trying: _START_ The noises stopped\n",
      "Ngram not found for: _START_ The noises stopped\n",
      "trying: The noises stopped as\n",
      "Ngram not found for: The noises stopped as\n",
      "trying: noises stopped as the\n",
      "Ngram not found for: noises stopped as the\n",
      "trying: stopped as the baby\n",
      "Ngram not found for: stopped as the baby\n",
      "trying: as the baby squirrels\n",
      "Ngram not found for: as the baby squirrels\n",
      "trying: the baby squirrels began\n",
      "Ngram not found for: the baby squirrels began\n",
      "trying: baby squirrels began to\n",
      "Ngram not found for: baby squirrels began to\n",
      "trying: squirrels began to eat\n",
      "Ngram not found for: squirrels began to eat\n",
      "trying: _START_ Sam and Jo\n",
      "Ngram not found for: _START_ Sam and Jo\n",
      "trying: Sam and Jo smiled\n",
      "Ngram not found for: Sam and Jo smiled\n",
      "trying: and Jo smiled _._\n",
      "Ngram not found for: and Jo smiled _._\n",
      "trying: Jo smiled _._ knowing\n",
      "Ngram not found for: Jo smiled _._ knowing\n",
      "trying: smiled _._ knowing the\n",
      "Ngram not found for: smiled _._ knowing the\n",
      "trying: _._ knowing the squirrels\n",
      "Ngram not found for: _._ knowing the squirrels\n",
      "trying: knowing the squirrels were\n",
      "Ngram not found for: knowing the squirrels were\n",
      "trying: the squirrels were safe\n",
      "Ngram not found for: the squirrels were safe\n",
      "trying: squirrels were safe with\n",
      "Ngram not found for: squirrels were safe with\n"
     ]
    }
   ],
   "source": [
    "rv_ngrams_2_1 = compute_ngrams_for_sentence(doc, 2, 1) # bigram: v v w v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv_ngrams_2_2 = compute_ngrams_for_sentence(doc, 2, 2) # bigram: v v w v v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature: IPA (very naive)\n",
    "\n",
    "- number of distinct glyphs in the IPA for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv_ipa = []\n",
    "for sent in sentences:\n",
    "    for word in sent:\n",
    "        if word in {'.', ',', '?', '!', '_START_'}:\n",
    "            rv_ipa.append(tuple([word, 0]))\n",
    "        else:\n",
    "            rv_ipa.append(tuple([word, len(set(eng_to_ipa.convert(word)))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_matrix = pd.DataFrame(f_trigram_likelihood)\n",
    "# text_matrix = text_matrix.rename(columns={0: 'token', 1: 'ngram-1-1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values_of = lambda x: [xy[1] for xy in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_matrix['ngrams-1-0'] = values_of(rv_ngrams_1_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_matrix['ngrams-2-0'] = values_of(rv_ngrams_2_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_matrix['ngrams-0-1'] = values_of(rv_ngrams_0_1)\n",
    "# text_matrix['ngrams-0-2'] = values_of(rv_ngrams_0_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_matrix['word-len'] = values_of(f_word_length)\n",
    "# text_matrix['sight-word'] = values_of(f_sight_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_matrix['naive-ipa'] = values_of(rv_ipa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_matrix.to_csv('output/20200623-text-matrix-330.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
