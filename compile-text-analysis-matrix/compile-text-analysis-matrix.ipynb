{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile text-analysis matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, json, string\n",
    "import text_analysis_util as TAU\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import CMUSounds\n",
    "import LTS_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_NUMBER = '330'\n",
    "OUTPUT_PREFIX = '20200820_'\n",
    "OUTPUT_PATH = '../output/text-analysis-matrix/'\n",
    "\n",
    "OUTPUT_FILE_PATH = OUTPUT_PATH + OUTPUT_PREFIX + ITEM_NUMBER + '.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PASSAGES_WITH_LINE_BREAKS = '../data/moby-passages-36/passages-with-line-breaks.tsv'\n",
    "PASSAGES_WITH_RECSTRING = '../data/moby-passages-36/passages-with-line-break-and-recstring.tsv'\n",
    "\n",
    "CMU = CMUSounds.CMUSounds('../data/general-resources/cmudict-0.7b.txt')\n",
    "LTS = LTS_util.LTS_util()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Works!\n"
    }
   ],
   "source": [
    "TEST_item_to_recstring = {}\n",
    "with open(PASSAGES_WITH_RECSTRING) as f:\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        _, _, item, _, recstring = line.strip().split('\\t')\n",
    "        # assert (recstring[0] == recstring[-1] == \"'\") or \\\n",
    "        #     (recstring[0] == recstring[-1] == '\"'), recstring\n",
    "        TEST_item_to_recstring[item] = recstring.split()\n",
    "\n",
    "item_to_rough_tokens = {}\n",
    "with open(PASSAGES_WITH_LINE_BREAKS) as f:\n",
    "    for line in f:\n",
    "        item, full_passage = line.strip().split('\\t')\n",
    "        assert (full_passage[0] == full_passage[-1] == \"'\") or \\\n",
    "            (full_passage[0] == full_passage[-1] == '\"'), 'Ill-formed passage'\n",
    "        full_passage = full_passage[1:-1]\n",
    "\n",
    "        tokenized_passage = TAU.text_analysis_tokenize(full_passage)\n",
    "        item_to_rough_tokens[item] = tokenized_passage\n",
    "\n",
    "        recstring = TAU.turn_tokens_to_recstring(tokenized_passage)\n",
    "        expected = TEST_item_to_recstring[item]\n",
    "\n",
    "        \n",
    "        expected2 = []\n",
    "        for token in tokenized_passage:\n",
    "            token_processed = TAU.turn_token_to_recstring_format(token)\n",
    "            if token_processed:\n",
    "                expected2.append(token_processed)\n",
    "\n",
    "        assert expected == recstring, (recstring, expected)\n",
    "        assert expected2 == recstring, (recstring, expected2)\n",
    "    print('Works!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for item, rough_tokens in item_to_rough_tokens.items():\n",
    "#     for token in rough_tokens:\n",
    "#         if TAU.given_word_test_if_word_is_end_of_sentence(token):\n",
    "#             print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step I.\n",
    "\n",
    "Collect source passage-dependent text-analysis features first (line break? paragraph break? preceded/followed by punctuation?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_passage_dependent_features(source_passage):\n",
    "    LAST_INDEX = len(source_passage) - 1\n",
    "    rv = []\n",
    "    for idx, token in enumerate(source_passage):\n",
    "        token_is_word = ('*' != token != '$')\n",
    "\n",
    "        if not token_is_word:\n",
    "            continue\n",
    "\n",
    "        # features\n",
    "        word_is_start_of_line      = False\n",
    "        word_is_end_of_line        = False\n",
    "        word_is_start_of_paragraph = False\n",
    "        word_is_end_of_paragraph   = False\n",
    "\n",
    "        word_contains_punctuation  = False\n",
    "        word_is_start_of_sentence  = False\n",
    "        word_is_end_of_sentence    = False\n",
    "\n",
    "        if idx == 0:\n",
    "            word_is_start_of_line      = True\n",
    "            word_is_start_of_paragraph = True\n",
    "            word_is_start_of_sentence  = True\n",
    "        \n",
    "        if idx == LAST_INDEX:\n",
    "            word_is_end_of_line        = True\n",
    "            word_is_end_of_paragraph   = True\n",
    "            word_is_end_of_sentence    = True\n",
    "\n",
    "        if idx != 0:\n",
    "            prev_token = source_passage[idx - 1]\n",
    "            if prev_token == '*':\n",
    "                prev_token = source_passage[idx - 2]\n",
    "                word_is_start_of_paragraph = True\n",
    "                word_is_start_of_line = True\n",
    "            if prev_token == '$':\n",
    "                prev_token = source_passage[idx - 2]\n",
    "                word_is_start_of_line = True\n",
    "            if TAU.given_word_test_if_word_is_end_of_sentence(prev_token):\n",
    "                word_is_start_of_sentence = True\n",
    "            \n",
    "        if idx != LAST_INDEX:\n",
    "            next_token = source_passage[idx + 1]\n",
    "            if next_token == '*':\n",
    "                word_is_end_of_paragraph = True\n",
    "                word_is_end_of_line = True\n",
    "            if next_token == '$':\n",
    "                word_is_end_of_line = True\n",
    "        \n",
    "        if TAU.given_word_test_if_word_is_end_of_sentence(token):\n",
    "            word_is_end_of_sentence = True\n",
    "        \n",
    "        if set(string.punctuation) & set(token):\n",
    "            word_contains_punctuation = True\n",
    "\n",
    "        features = {\n",
    "            'word_is_start_of_line': word_is_start_of_line,\n",
    "            'word_is_end_of_line': word_is_end_of_line,\n",
    "            'word_is_start_of_paragraph': word_is_start_of_paragraph,\n",
    "            'word_is_end_of_paragraph': word_is_end_of_paragraph,\n",
    "            'word_contains_punctuation': word_contains_punctuation,\n",
    "            'word_is_start_of_sentence': word_is_start_of_sentence,\n",
    "            'word_is_end_of_sentence': word_is_end_of_sentence,\n",
    "        }\n",
    "        token_processed = TAU.turn_token_to_recstring_format(token)\n",
    "        rv.append((token_processed, features))\n",
    "    return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_passage_rough_tokens = item_to_rough_tokens['330']\n",
    "TEST_passage_feature_list = get_source_passage_dependent_features(TEST_passage_rough_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(data, feature_function, feature_name):\n",
    "    rv = data\n",
    "    for idx, (word, existing_features) in enumerate(data):\n",
    "        feature_value_to_add = feature_function(word)\n",
    "        existing_features[feature_name] = feature_value_to_add\n",
    "        rv[idx] = (word, existing_features)\n",
    "    return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FEATURE_FN_WORD_LENGTH(word):\n",
    "    word_with_only_letters = [letter for letter in word if letter.isalpha()]\n",
    "    return len(word_with_only_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FEATURE_FN_CMU_LENGTH(word):\n",
    "    return len(CMU.get(word).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def FEATURE_FN_LTS_1(word):\n",
    "    return 1 in LTS.get(word)['lts']\n",
    "    \n",
    "def FEATURE_FN_LTS_2(word):\n",
    "    return 2 in LTS.get(word)['lts']\n",
    "    \n",
    "def FEATURE_FN_LTS_3(word):\n",
    "    return 3 in LTS.get(word)['lts']\n",
    "    \n",
    "def FEATURE_FN_LTS_4(word):\n",
    "    return 4 in LTS.get(word)['lts']\n",
    "    \n",
    "def FEATURE_FN_LTS_5(word):\n",
    "    return 5 in LTS.get(word)['lts']\n",
    "    \n",
    "def FEATURE_FN_LTS_6(word):\n",
    "    return 6 in LTS.get(word)['lts']\n",
    "    \n",
    "def FEATURE_FN_LTS_7(word):\n",
    "    return 7 in LTS.get(word)['lts']\n",
    "    \n",
    "def FEATURE_FN_LTS_8(word):\n",
    "    return 8 in LTS.get(word)['lts']\n",
    "    \n",
    "def FEATURE_FN_LTS_9(word):\n",
    "    return 9 in LTS.get(word)['lts']\n",
    "    \n",
    "def FEATURE_FN_LTS_10(word):\n",
    "    return 10 in LTS.get(word)['lts']\n",
    "    \n",
    "def FEATURE_FN_LTS_11(word):\n",
    "    return 11 in LTS.get(word)['lts']\n",
    "    \n",
    "def FEATURE_FN_LTS_12(word):\n",
    "    return 12 in LTS.get(word)['lts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FEATURE_FN_MORPHS(word):\n",
    "    return LTS.get(word)['n_morphs']\n",
    "    \n",
    "def FEATURE_FN_DECODABLE(word):\n",
    "    return LTS.get(word)['decodable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FEATURE_FN_SIGHTWORD_PP(word):\n",
    "    return -1 in LTS.get(word)['sight_word']\n",
    "\n",
    "def FEATURE_FN_SIGHTWORD_P(word):\n",
    "    return 0 in LTS.get(word)['sight_word'] or -1 in LTS.get(word)['sight_word']\n",
    "\n",
    "def FEATURE_FN_SIGHTWORD_1(word):\n",
    "    return LTS.get(word)['sight_word'] != []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "PASSAGE_ROUGH_TOKENS = item_to_rough_tokens[ITEM_NUMBER]\n",
    "PASSAGE_TOKENS_AND_FEATURE_LISTS = get_source_passage_dependent_features(PASSAGE_ROUGH_TOKENS)\n",
    "\n",
    "FEATURES_TO_ADD = [\n",
    "    (FEATURE_FN_WORD_LENGTH, 'word_length'),\n",
    "    (FEATURE_FN_CMU_LENGTH, 'CMU_length'),\n",
    "    (FEATURE_FN_LTS_1, 'lts_1'),\n",
    "    (FEATURE_FN_LTS_2, 'lts_2'),\n",
    "    (FEATURE_FN_LTS_3, 'lts_3'),\n",
    "    (FEATURE_FN_LTS_4, 'lts_4'),\n",
    "    (FEATURE_FN_LTS_5, 'lts_5'),\n",
    "    (FEATURE_FN_LTS_6, 'lts_6'),\n",
    "    (FEATURE_FN_LTS_7, 'lts_7'),\n",
    "    (FEATURE_FN_LTS_8, 'lts_8'),\n",
    "    (FEATURE_FN_LTS_9, 'lts_9'),\n",
    "    (FEATURE_FN_LTS_10, 'lts_10'),\n",
    "    (FEATURE_FN_LTS_11, 'lts_11'),\n",
    "    (FEATURE_FN_LTS_12, 'lts_12'),\n",
    "    (FEATURE_FN_MORPHS, 'n_morphs'),\n",
    "    (FEATURE_FN_DECODABLE, 'is_decodable'),\n",
    "    (FEATURE_FN_SIGHTWORD_PP, 'sightword_pp'),\n",
    "    (FEATURE_FN_SIGHTWORD_P, 'sightword_p'),\n",
    "    (FEATURE_FN_SIGHTWORD_1, 'sightword_1')\n",
    "]\n",
    "\n",
    "for feature_fn, feature_fn_name in FEATURES_TO_ADD:\n",
    "    PASSAGE_TOKENS_AND_FEATURE_LISTS = add_feature(PASSAGE_TOKENS_AND_FEATURE_LISTS, feature_fn, feature_fn_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "PASSAGE_TOKENS_ONLY = [x[0] for x in PASSAGE_TOKENS_AND_FEATURE_LISTS]\n",
    "PASSAGE_FEATURES = [x[1] for x in PASSAGE_TOKENS_AND_FEATURE_LISTS]\n",
    "PASSAGE_DF = pd.DataFrame(PASSAGE_FEATURES)\n",
    "PASSAGE_DF.index = PASSAGE_TOKENS_ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "PASSAGE_DF.to_csv(OUTPUT_FILE_PATH, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}